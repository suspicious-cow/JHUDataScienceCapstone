---
title: "Johns Hopkins University Data Science Capstone - Milestone Report"
author: "Zain Naboulsi"
date: "2023-05-13"
bibliography: references.bib
output:
  html_document:
    
    toc: yes
  pdf_document:
    
    toc: yes
editor_options: 
  markdown: 
    
    wrap: 80
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}
# run our setup code to obtain all the objects we will need to produce our data and graphs

##
# Created by Zain Naboulsi for the Johns Hopkins Data Science Specialization Capstone Course 
##

# set a seed in case we use anything that uses random numbers
set.seed(1337)


# Set the names of the packages and libraries you want to install
# Most notably load up all the quanteda packages we will need
required_libraries <- c("quanteda","quanteda.textmodels","quanteda.textstats",
                        "quanteda.textplots", "readtext", "text", "sqldf", 
                        "digest", "ggplot2", "dplyr", "gridExtra")

for (lib in required_libraries) {
  if (!requireNamespace(lib, quietly = TRUE)) {
    install.packages(lib)
  }
  library(lib, character.only = TRUE)
}


# Get our initial file metrics on the original source files
# see if we have an RDS file for the object we need
if (file.exists("Objects/initialfilemetrics.rds")) {
  
  # if we do, read our object into memory from disk
  initialfilemetrics <- readRDS("Objects/initialfilemetrics.rds")
  
} else {
  # Define our file paths
  file_paths <- c("SwiftKeyData/en_US.blogs.txt", "SwiftKeyData/en_US.news.txt", "SwiftKeyData/en_US.twitter.txt")
  
  # Initialize a data frame to store the results
  initialfilemetrics <- data.frame(
    File = character(),
    Line_Count = numeric(),
    Word_Count = numeric(),
    stringsAsFactors = FALSE
  )
  
  # Process each file in our loop
  for (file_path in file_paths) {
    
    # Initialize counters
    line_count <- 0
    word_count <- 0
    
    # Open a connection to the file
    con <- file(file_path, "r")
    
    # Read the file line by line
    while (TRUE) {
      lines <- readLines(con, n = 1)  # read one line at a time
      
      if (length(lines) == 0) {  # end of file
        break
      }
      
      # Update line and word counts
      line_count <- line_count + 1
      word_count <- word_count + length(strsplit(lines, "\\s")[[1]])
    }
    
    # Close the connection
    close(con)
    
    # Add the results to the data frame for each file
    initialfilemetrics <- rbind(initialfilemetrics, data.frame(
      File = file_path,
      Line_Count = line_count,
      Word_Count = word_count,
      stringsAsFactors = FALSE
    ))
  }
  
  # save our object to disk
  saveRDS(initialfilemetrics, "Objects/initialfilemetrics.rds")
}

##
# Created by Zain Naboulsi for the Johns Hopkins Data Science Specialization Capstone Course 
##

# Check if the digest files are missing
if (!file.exists("Digests/blogs_sample_digest.txt") | 
    !file.exists("Digests/news_sample_digest.txt") | 
    !file.exists("Digests/twitter_sample_digest.txt")) {
  
  # if any digest file is missing then create them all to keep track of 
  # when the data changes
  blogs_sample_digest <- digest(file.info("SampleData/blogs_sample.txt")$mtime)
  write.table(blogs_sample_digest, file = "Digests/blogs_sample_digest.txt", 
              row.names = FALSE, col.names = FALSE, quote = FALSE, append = FALSE)
  
  news_sample_digest <- digest(file.info("SampleData/news_sample.txt")$mtime)
  write.table(news_sample_digest, file = "Digests/news_sample_digest.txt", 
              row.names = FALSE, col.names = FALSE, quote = FALSE, append = FALSE)
  
  twitter_sample_digest <- digest(file.info("SampleData/twitter_sample.txt")$mtime)
  write.table(twitter_sample_digest, file = "Digests/twitter_sample_digest.txt", 
              row.names = FALSE, col.names = FALSE, quote = FALSE, append = FALSE)
  
} else {
  
  # if no files are missing read in the digest information for all three files
  blogs_sample_digest <- readLines("Digests/blogs_sample_digest.txt")
  news_sample_digest <- readLines("Digests/news_sample_digest.txt")
  twitter_sample_digest <- readLines("Digests/twitter_sample_digest.txt")
}


# Define the function for sampling lines to use if we have changes to the data
sample_lines2 <- function(file, n) {
  total_lines <- sum(readLines(file) != "")
  p <- n / total_lines
  lines_sample <- c()
  con <- file(file, "r")
  while(TRUE) {
    line <- readLines(con, n = 1)
    if(length(line) == 0) {
      break
    }
    if(runif(1) < p) {
      lines_sample <- c(lines_sample, line)
    }
  }
  close(con)
  return(lines_sample)
}


# Check if the sample files have changed by looking at the modification times
if (digest(file.info("SampleData/blogs_sample.txt")$mtime) != blogs_sample_digest |
    digest(file.info("SampleData/news_sample.txt")$mtime) != news_sample_digest |
    digest(file.info("SampleData/twitter_sample.txt")$mtime) != twitter_sample_digest) {
  
  # redo the seed to make sure the results are reproducible
  set.seed(1337)
  
  # if there have been changes to the files then we need to sample the 
  # lines from the new files again
  sample_size <- 50000
  blogs_sample <- sample_lines2("SwiftKeyData/en_US.blogs.txt", sample_size)
  news_sample <- sample_lines2("SwiftKeyData/en_US.news.txt", sample_size)
  twitter_sample <- sample_lines2("SwiftKeyData/en_US.twitter.txt", sample_size)
  
  # now let's save our sample files to disk
  write.table(blogs_sample, file = "SampleData/blogs_sample.txt", 
              row.names = FALSE, col.names = FALSE, quote = FALSE, append = FALSE)
  write.table(news_sample, file = "SampleData/news_sample.txt", 
              row.names = FALSE, col.names = FALSE, quote = FALSE, append = FALSE)
  write.table(twitter_sample, file = "SampleData/twitter_sample.txt", 
              row.names = FALSE, col.names = FALSE, quote = FALSE, append = FALSE)
  
} else {
  
  # if none of the sample files have changed just use the already generated
  # sample files
  blogs_sample <- readLines("SampleData/blogs_sample.txt")
  news_sample <- readLines("SampleData/news_sample.txt")
  twitter_sample <- readLines("SampleData/twitter_sample.txt")
}

# after everything is done, combine our three sample files into one
full_data_sample <- c(blogs_sample, news_sample, twitter_sample)

# Check if all of our saved objects exist
if (file.exists("Objects/corp.rds") &
    file.exists("Objects/tokens.rds") &
    file.exists("Objects/tokens_dfm.rds") &
    file.exists("Objects/ngram2.rds") &
    file.exists("Objects/ngram3.rds") &
    file.exists("Objects/ngram4.rds") & 
    file.exists("Objects/collocations2.rds") &
    file.exists("Objects/collocations3.rds") &
    file.exists("Objects/collocations4.rds") &
    file.exists("Objects/tokens_nostop.rds") &
    file.exists("Objects/tokens_dfm_nostop.rds") &
    file.exists("Objects/ngram2_nostop.rds") &
    file.exists("Objects/ngram3_nostop.rds") &
    file.exists("Objects/ngram4_nostop.rds") & 
    file.exists("Objects/collocations2_nostop.rds") &
    file.exists("Objects/collocations3_nostop.rds") &
    file.exists("Objects/collocations4_nostop.rds") 
    
    
    ) {
  
  # if all of our saved objects are there then load them into memory
  corp <- readRDS("Objects/corp.rds")
  tokens <- readRDS("Objects/tokens.rds")
  tokens_dfm <- readRDS("Objects/tokens_dfm.rds")
  ngram2 <- readRDS("Objects/ngram2.rds")
  ngram3 <- readRDS("Objects/ngram3.rds")
  ngram4 <- readRDS("Objects/ngram4.rds")
  collocations2 <- readRDS("Objects/collocations2.rds")
  collocations3 <- readRDS("Objects/collocations3.rds")
  collocations3 <- readRDS("Objects/collocations4.rds")
  tokens_nostop <- readRDS("Objects/tokens_nostop.rds")
  tokens_dfm_nostop <- readRDS("Objects/tokens_dfm_nostop.rds")
  ngram2_nostop <- readRDS("Objects/ngram2_nostop.rds")
  ngram3_nostop <- readRDS("Objects/ngram3_nostop.rds")
  ngram4_nostop <- readRDS("Objects/ngram4_nostop.rds")
  collocations2_nostop <- readRDS("Objects/collocations2_nostop.rds")
  collocations3_nostop <- readRDS("Objects/collocations3_nostop.rds")
  collocations3_nostop <- readRDS("Objects/collocations4_nostop.rds")
  
} else {
  
  # if any saved objects are missing, then redo all the objects just to make
  # sure we don't miss anything
  
  # Create the corpus and tokens objects
  corp <- corpus(full_data_sample)
  tokens <- tokens(corp)
  
  # remove all non-alphanumeric characters since they aren't interesting to us
  tokens <- tokens_remove(tokens, pattern = "[^[:alnum:]]", valuetype = "regex") 
  tokens_dfm <- dfm(tokens)
  
  # generate our frequency distributions for bi,tri, and quad-grams
  # with stopwords
  ngram2 <- tokens_ngrams(tokens, n = 2)
  ngram3 <- tokens_ngrams(tokens, n = 3)
  ngram4 <- tokens_ngrams(tokens, n = 4)
  
  
  # create our bigram and trigram objects for sample with stopwords
  collocations2 <- textstat_collocations(tokens, size = 2)  # for bi-grams
  collocations3 <- textstat_collocations(tokens, size = 3)  # for tri-grams
  collocations4 <- textstat_collocations(tokens, size = 4)  # for quad-grams
  
  # create tokens and dfm with stopwords removed
  tokens_nostop <- tokens_remove(tokens, pattern = stopwords("english"))
  tokens_dfm_nostop <- dfm(tokens_nostop)
  
  # generate our frequency distributions for bi,tri, and quad-grams
  # without stopwords
  ngram2_nostop <- tokens_ngrams(tokens_nostop, n = 2)
  ngram3_nostop <- tokens_ngrams(tokens_nostop, n = 3)
  ngram4_nostop <- tokens_ngrams(tokens_nostop, n = 4)
  
  # create our bigram and trigram objects for sample without stopwords
  collocations2_nostop <- textstat_collocations(tokens_nostop, size = 2)  # for bi-grams
  collocations3_nostop <- textstat_collocations(tokens_nostop, size = 3)  # for tri-grams
  collocations4_nostop <- textstat_collocations(tokens_nostop, size = 4)  # for quad-grams
  
  # Save the corpus, tokens, ngrams, and collocations to RDS files
  saveRDS(corp, "Objects/corp.rds")
  saveRDS(tokens, "Objects/tokens.rds")
  saveRDS(tokens_dfm, "Objects/tokens_dfm.rds")
  saveRDS(ngram2, "Objects/ngram2.rds")
  saveRDS(ngram3, "Objects/ngram3.rds")
  saveRDS(ngram4, "Objects/ngram4.rds")
  saveRDS(collocations2, "Objects/collocations2.rds")
  saveRDS(collocations3, "Objects/collocations3.rds")
  saveRDS(collocations4, "Objects/collocations4.rds")
  saveRDS(tokens_nostop, "Objects/tokens_nostop.rds")
  saveRDS(tokens_dfm_nostop, "Objects/tokens_dfm_nostop.rds")
  saveRDS(ngram2_nostop, "Objects/ngram2_nostop.rds")
  saveRDS(ngram3_nostop, "Objects/ngram3_nostop.rds")
  saveRDS(ngram4_nostop, "Objects/ngram4_nostop.rds")
  saveRDS(collocations2_nostop, "Objects/collocations2_nostop.rds")
  saveRDS(collocations3_nostop, "Objects/collocations3_nostop.rds")
  saveRDS(collocations4_nostop, "Objects/collocations4_nostop.rds")
}

# Update the digest files with the new modification times
blogs_sample_digest <- digest(file.info("SampleData/blogs_sample.txt")$mtime)
news_sample_digest <- digest(file.info("SampleData/news_sample.txt")$mtime)
twitter_sample_digest <- digest(file.info("SampleData/twitter_sample.txt")$mtime)
full_data_sample_digest <- digest(file.info("SampleData/full_data_sample.txt")$mtime)

writeLines(blogs_sample_digest, con = "SampleData/blogs_sample_digest.txt")
writeLines(news_sample_digest, con = "SampleData/news_sample_digest.txt")
writeLines(twitter_sample_digest, con = "SampleData/twitter_sample_digest.txt")
writeLines(full_data_sample_digest, con = "SampleData/full_data_sample_digest.txt")


##
# Created by Zain Naboulsi for the Johns Hopkins Data Science Specialization Capstone Course 
##


```

# Introduction
Welcome to the Milestone Report for the Johns Hopkins Data Science 
Specialization. We appreciate you taking time to review our work and give 
feedback on our progress as well as thoughts on fitting a prediction model. 
Also, any insight you would like to share on the final plan for our Shiny 
application would be welcome. 

# Initial Data Load  

We started by taking three original text files, provided by SwiftKey, that were 
taken from blogs, news, and Twitter entries on the Internet. Initially, we had 
planned to do analysis on the entire set of data; however that idea quickly 
dissolved as our first attempt generating counts of each word took over sixteen 
hours to complete. Naturally, this was a non-starter. 

We then turned our full attention to the total size of the original source files 
to get a handle on what we should do with them:

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE}
print(initialfilemetrics)
```
As you can see the files are quite large with a vast amount of data in them. 
Their size, however, precludes us being able to have reasonable performance when 
running our analysis against them. We need to reduce the size of the data being 
analyzed. To do this, we opted to take a random sample of fifty thousand lines 
from each of the original files (blogs, news, and Twitter). 

# Analysis  

## Stopwords

At this point we were ready to do some analysis but we first had to make a 
critical decision: Should we include stopwords? Traditionally, stopwords such as 
"an", "the", "or", etc... are removed for analysis. However, our goal is to 
create a prediction model on the next word(s) that will occur in a sequence. 
Stopwords are an integral part of sentence structure and, therefore, should 
probably be left in for analysis. For the time being, we decided to do 
preliminary analysis on data with and without stopwords for comparison. 

## Data Sample Files Validation
To validate our sample files we used bootstrapping, a statistical resampling 
method, to understand the distribution of unique words in our three types of 
samples: blog posts, news articles, and Twitter messages. Bootstrapping lets us 
draw multiple samples from our original data. By doing this 5000 times for each 
data file sample, we can calculate the average (mean) and variability (standard 
deviation) of unique words in each sample.

```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE, cache = TRUE}

# check and see if our bootstap object is already saved
if (file.exists("Objects/bootstrap_results_list.rds")) {
  
  # if it is then read our bootstrap object into memory from disk
  bootstrap_results_list <- readRDS("Objects/bootstrap_results_list.rds")
  
} else {
  # Function to process a file and generate bootstrap samples
  bootstrap_unique_words <- function(file_path, n_bootstrap) {
    # Read the file
    text <- readLines(file_path, warn = FALSE)
    
    # Tokenize the text
    tokens <- tokens(text, what = "word", remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
      tokens_tolower() %>%
      tokens_remove(stopwords("english"))
    
    # Generate bootstrap samples and compute the number of unique tokens
    n_unique <- numeric(n_bootstrap)
    for (i in 1:n_bootstrap) {
      bootstrap_sample <- sample(tokens, size = length(tokens), replace = TRUE)
      n_unique[i] <- ntype(bootstrap_sample)
    }
    
    return(n_unique)
  }
  
  
# Define our file paths
file_paths <- c("SampleData/blogs_sample.txt", "SampleData/news_sample.txt", "SampleData/twitter_sample.txt")  


# Number of bootstrap samples
n_bootstrap <- 5000

# Perform bootstrap sampling for each file
bootstrap_results_list <- lapply(file_paths, bootstrap_unique_words, n_bootstrap = n_bootstrap)

# save the bootstrap_results_list object to disk
saveRDS(bootstrap_results_list, "Objects/bootstrap_results_list.rds")
  
}



# Combine the results into a data frame for plotting
bootstrap_results_df <- data.frame(
  Blogs = bootstrap_results_list[[1]],
  News = bootstrap_results_list[[2]],
  Twitter = bootstrap_results_list[[3]]
)

# Melt the data frame to long format
bootstrap_results_long <- reshape2::melt(bootstrap_results_df)

# Compute the mean and standard deviation of the number of unique words for each source file
summary_stats <- sapply(bootstrap_results_list, function(x) c(mean = mean(x), sd = sd(x)))

# Convert to a data frame for easier viewing
summary_stats_df <- data.frame(t(summary_stats))
names(summary_stats_df) <- c("Mean", "Standard Deviation")
rownames(summary_stats_df) <- c("Blogs", "News", "Twitter")

cat("Summary Statistics Without Stopwords", "\n")
print(summary_stats_df)


```


These results suggest that blog and news data tend to have a higher average 
number of unique words and more variability compared to Twitter, which shows 
greater consistency, possibly due to its character limit. Remember, the validity 
of these bootstrap samples hinges on the original samples being representative 
of all blog posts, news articles, and tweets. Bias in the original sample will 
also be reflected in the bootstrap samples.

The below graph, a set of histograms, a snapshot of the variety of words in our 
sample files from blogs, news, and Twitter. It helps us visualize and understand 
the patterns and variability in our data. Each bar in the histogram represents 
the frequency of a specific range of unique word counts from our bootstrap 
samples. By looking at these bars, we can see the most common outcomes (where 
the bars are highest), how spread out the outcomes are, and whether the spread 
is even or leans more one way or the other.

The shape of these histograms can tell us a lot. For example, the histograms for 
blogs and news looks different from the one for Twitter. This suggests that the 
there are different patterns of word use. Where blogs and news have similar 
patterns and, therefore, word use; Twitter does not follow this pattern. This 
could be because of the nature of the content, or even Twitter's character 
limit. In short, these histograms not only help validate our samples but also 
offer insights into the differences in language use across these platforms. 


```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE, cache = TRUE}
ggplot(bootstrap_results_long, aes(x = value, fill = variable)) + 
  geom_histogram(bins = 30, color = "black", alpha = 0.7) +  
  scale_fill_manual(values = c("#F8766D", "#00BA38", "#619CFF")) +  
  facet_wrap(~ variable, scales = "free_x") +
  theme_minimal() +
  theme(
    text = element_text(size = 14),  
    legend.position = "bottom",  
    plot.title = element_text(hjust = 0.5)  
  ) +
  labs(
    title = "Bootstrap Sample Unique Word Counts (No Stopwords)", 
    y = "Frequency", 
    x = "Number of Unique Words", 
    fill = "Text Source"  
  )

```


## Combining the Samples
Having validated our samples individually, we combined them into one file to 
facilitate fitting into our model. Our exploration begins with understanding our 
data, focusing on the frequency of words. Two histograms are created to give us 
a birds-eye view of this frequency distribution. The first visual dives into the 
data in its raw form, including stopwords. By doing this, we're keeping a record 
of the rhythm of language usage, but it may not give us significant insights 
into the specific content.

The second visual takes a different approach. We filter out stopwords, allowing 
the content-specific words to take the spotlight. This histogram then represents 
the frequency of these words, unveiling the most common themes or topics in our 
data. This rigorous approach to study word frequency, with and without 
stopwords, is a cornerstone of text analysis, and allows us to garner a more 
profound understanding of the data. 
```{r, echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, appendix = TRUE, cache = TRUE}
# Generate frequency histogram of data with stopwords

# Extract word frequencies
freq <- textstat_frequency(tokens_dfm)

# Sort in descending order and keep only the top 30 words
top_words <- head(freq, 30)

# Show our histogram
ggplot(top_words, aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 30 Word Frequencies with Stopwords",
       x = "Words",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 20), 
        axis.text.y = element_text(size = 9), 
        axis.text.x = element_text(size = 12),
        axis.title = element_text(size = 14))


# Generate frequency histogram of data without stopwords

# Extract word frequencies
freq_nostop <- textstat_frequency(tokens_dfm_nostop)

# Sort in descending order and keep only the top 30 words
top_words_nostop <- head(freq_nostop, 30)


# Show our histogram
ggplot(top_words_nostop, aes(x = reorder(feature, -frequency), y = frequency)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 30 Word Frequencies without Stopwords",
       x = "Words",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, size = 20), 
        axis.text.y = element_text(size = 9), 
        axis.text.x = element_text(size = 12),
        axis.title = element_text(size = 14))

```




\newpage
# References  

<div id="refs"></div>  


# Appendix: All Source Code  

```{r ref.label = knitr::all_labels(appendix == TRUE), echo=TRUE, eval=FALSE}
```